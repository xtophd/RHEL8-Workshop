:sectnums:
:sectnumlevels: 3
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]


:toc:
:toclevels: 1

= Boom

BOOM is a boot manager for Linux systems that support the 'BootLoader Specification' for boot entry configuration (ie: RHEL8 with GRUB2). It simplifies the creation of new or modified boot entries: for example, to boot snapshot images of the system created using LVM.

BOOM does not modify the existing boot loader configuration, and *only inserts* additional entries. The existing configuration is maintained, and any distribution integration, such as kernel installation and update scripts, continue to function as before.

BOOM also has a simple command-line interface (CLI) and API.

== Getting Started

Starting on the host *workstation.example.com*, let's ssh over to *node3.example.com*.  No password should be required.

.[root@workstation]#
----
ssh node3.example.com
----

Verify that you are on the right host for these exercises.

.[root@node3]#
----
cheat-boom-checkhost.sh
----

Now you are ready to begin

== Installation

The following set of instructions will install Boom.

.[root@node3]# 
----
yum install -y boom-boot
----

That was easy!

== Add Capacity to Root LVM

In order to support LVM snapshots on the root volume, we need to:
  * add some additional storage capacity
  * partition the disk
  * add the partition to the root volume group
  * and finally create logical volume to support the snapshot
  
  The following command will take care of all of these steps for you.  The individual steps are also outlined below.

.[root@node3]#
----
cheat-boom-addstorage.sh
----

[NOTE]
====
_Native command(s) to add storage_
----
printf "o\nn\np\n\n\n\nw\n" | fdisk /dev/vdb

pvcreate /dev/vdb1

vgextend rhel /dev/vdb1
----
====

.Command Output
[source, indent=4]
----
Clearing /dev/vdb
Creating new partition table on /dev/vdb

Welcome to fdisk (util-linux 2.32.1).
Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

Device does not contain a recognized partition table.
Created a new DOS disklabel with disk identifier 0xb8fd7055.

Command (m for help): Created a new DOS disklabel with disk identifier 0x4e3b8150.

Command (m for help): Partition type
   p   primary (0 primary, 0 extended, 4 free)
   e   extended (container for logical partitions)
Select (default p): Partition number (1-4, default 1): First sector (2048-20971519, default 2048): Last sector, +sectors or +size{K,M,G,T,P} (2048-20971519, default 20971519):
Created a new partition 1 of type 'Linux' and of size 10 GiB.

Command (m for help): The partition table has been altered.
Calling ioctl() to re-read partition table.
Syncing disks.

Creating physical volume lable on /dev/vdb1
  Physical volume "/dev/vdb1" successfully created.
Extending volume group 'rhel' with /dev/vdb1
  Volume group "rhel" successfully extended
  
  Reducing COW size 10.00 GiB down to maximum usable size <8.03 GiB.
  Logical volume "root_snapshot" created.
----

== Create some Important Stuff

The next set of instructions will create a file and a userID that we will remove and recover as part of this exercise:

.[root@node3]#
----
cheat-boom-importantstuff.sh
----

[NOTE]
====
_Native command(s) to create important file_
----
echo "This is an important file!!" > /root/importantfile

useradd importantuser

cat /root/importantfile

grep importantuser /etc/passwd
----
====

.Command Output
[source,indent=4]
----
Contents of 'importantfile':
This is an important file!!

Grepping 'importantuser' from /etc/passwd:
importantuser:x:1001:1001::/home/importantuser:/bin/bash
----

== Create LV Snapshot of Root Volume

Now that we have created some important data, it's time to create a snapshot of our system.

.[root@node3]#
----
cheat-boom-snapshot.sh
----

[NOTE]
====
_Native command(s) to create lvm snapshot_
----
lvcreate -s rhel/root -n root_snapshot -L 10G
----
====

Let us take a quick look at what we just did.

.[root@node3]#
----
cheat-boom-lvreport.sh
----

[NOTE]
====
_Native command(s) to report lv status_
----
lvs -a -o lv_name,lv_size,lv_attr,segtype,devices

lvs -a -o lv_name,lv_size,lv_attr,origin,snap_percent
----
====

This script uses different options to create 2 different reports.

.Command Output
[source,indent=4]
----
Local volume reports:
  LV            LSize  Attr       Type   Devices
  root          <8.00g owi-aos--- linear /dev/vda2(256)
  root_snapshot <8.03g swi-a-s--- linear /dev/vdb1(0)
  swap           1.00g -wi-ao---- linear /dev/vda2(0)

  LV            LSize  Attr       Origin Snap%
  root          <8.00g owi-aos---
  root_snapshot <8.03g swi-a-s--- root   0.01
  swap           1.00g -wi-ao----
----

In your output, note the percentage used in the new snapshot as shown by the 2nd report.

Since we have made no real changes to our system, the percentage of the "exception store" used is still very small.  Let's change that.

== Make Changes to System

Now we are going to remove the “importantfile” file and “importantuser” userID.

.[root@node3]#
----
rm -f /root/importantfile 
userdel importantuser
----

What the heck, let's anti up and delete some more stuff.  Who needs documentation anyway!

.[root@node3]#
----
rm -rf /usr/share/man
rm -rf /usr/share/doc
rm -rf /usr/share/GeoIP
----

Confirm that our changes were effective

.[root@node3]#
----
cat /root/importantfile

grep -c importantuser /etc/passwd
----

NOTE: "grep -c" counts how many time the token is matched.  In our case it should be zero

.Command Output
[source,indent=4]
----
cat: /root/importantfile: No such file or directory

0
----

Analyze the snapshot data and we see that there is now a measurable difference.

.[root@node3]#
----
cheat-boom-lvreport.sh
----

.Command Output
[source,indent=4]
----
Local volume reports:
  LV            LSize  Attr       Type   Devices
  root          <8.00g owi-aos--- linear /dev/vda2(256)
  root_snapshot <8.03g swi-a-s--- linear /dev/vdb1(0)
  swap           1.00g -wi-ao---- linear /dev/vda2(0)

  LV            LSize  Attr       Origin Snap%
  root          <8.00g owi-aos---
  root_snapshot <8.03g swi-a-s--- root   0.38
  swap           1.00g -wi-ao----
----

Let's summarize what's been done so far:

  * you added some storage capacity to the root volume
  * you created an "importantfile" and an "importantuser" on your host
  * you created a snapshot of the root volume
  * you then made some changes to the host (deleted a bunch of stuff)

Time to boot our host using the snapshot value, inspect the host to verify our data is there and then finally recover the host.

== BOOM!!!

Create a boom profile.

.[root@node3]#
----
cheat-boom-mkprofile.sh
----

[NOTE]
====
_Native command(s) to create boom profile_
----
boom profile create --from-host --uname-pattern el8
----
====

.Command Output
[source,indent=4]
----
Created profile with os_id e6f881a:
  OS ID: "e6f881ae3f8a2e010375fb840bb4f386b330db6e",
  Name: "Red Hat Enterprise Linux", Short name: "rhel",
  Version: "8.0 (Ootpa)", Version ID: "8.0",
  UTS release pattern: "el8",
  Kernel pattern: "/vmlinuz-%{version}", Initramfs pattern: "/initramfs-%{version}.img",
  Root options (LVM2): "rd.lvm.lv=%{lvm_root_lv}",
  Root options (BTRFS): "rootflags=%{btrfs_subvolume}",
  Options: "root=%{root_device} ro %{root_opts}"
----

Verify that the boom profile was created by the previous command.

.[root@node3]#
----
boom profile list
----

.Command Output
[source,indent=4]
----
OsID    Name                     OsVersion
e6f881a Red Hat Enterprise Linux 8.0 (Ootpa)
----

Now to create a boot entry for grub which utilizes the snapshot as the boot volume.

.[root@node3]#
----
cheat-boom-mkgrub.sh
----

[NOTE]
====
_Native command(s) to create grub entry_
----
boom create --title "root LV snapshot" --rootlv rhel/root_snapshot
----
====

.Command Output
[source,indent=4]
----
WARNING - Boom grub2 script missing from '/etc/grub.d'
WARNING - Boom configuration not found in grub.cfg
WARNING - Run 'grub2-mkconfig > /boot/grub2/grub.cfg' to enable
Created entry with boot_id 85e739d:
  title root LV snapshot
  machine-id e988045b45b04b11b84741d6a568861b
  version 4.18.0-67.el8.x86_64
  linux /vmlinuz-4.18.0-67.el8.x86_64
  initrd /initramfs-4.18.0-67.el8.x86_64.img
  options root=/dev/rhel/root_snapshot ro rd.lvm.lv=rhel/root_snapshot
----

Take a look at currently configured boom-boot entries.

.[root@node3]#
----
boom entry list
----

Your output should look like this.

[source,indent=4]
----
BootID  Version                  Name                     RootDevice
85e739d 4.18.0-67.el8.x86_64     Red Hat Enterprise Linux /dev/rhel/root_snapshot
----

Show details about our boom-boot entry.

.[root@node3]#
----
boom entry show 85e739d
----

.Command Output
[source,indent=4]
----
Boot Entry (boot_id=85e739d)
  title root LV snapshot
  machine-id e988045b45b04b11b84741d6a568861b
  version 4.18.0-67.el8.x86_64
  linux /vmlinuz-4.18.0-67.el8.x86_64
  initrd /initramfs-4.18.0-67.el8.x86_64.img
  options root=/dev/rhel/root_snapshot ro rd.lvm.lv=rhel/root_snapshot
----



== Reboot to Snapshot

WARNING: Bring up the virtual machine console for node3 before proceeding.  

Before reboot, there are 2 options to invoke the right loader at restart:
  . enter the GRUB menu and select at boot time
  . use grub-set-default to preselect which one to load
  
We are going to opt for preselect since it's just easier.  Use the following cheat to inspect the currently configured GRUB menu options.

.[root@node3]#
----
cheat-boom-grublist.sh
----

.Command Output
[source,indent=4]
----
0  title="root LV snapshot"
1  title="Red Hat Enterprise Linux (4.18.0-67.el8.x86_64) 8.0 (Ootpa)"
2  title="Red Hat Enterprise Linux (0-rescue-e988045b45b04b11b84741d6a568861b) 8.0 (Ootpa)"
----

We want to reboot to our snapshot, so in this case we use '0'.

.[root@node3]#
----
grub2-set-default 0
----

Verify that the parameters stuck

.[root@node3]#
----
grub2-editenv list
----

Notice that "saved_entry=0", that's what we want.

.Command Output
[source,indent=4]
----
saved_entry=0
kernelopts=root=/dev/mapper/rhel-root_snapshot ro crashkernel=auto resume=/dev/mapper/rhel-swap rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet
boot_success=0
----

We will now reset our host and boot the snapshot Logical Volume.

.[root@node3]#
----
reboot
----

=== Confirm Previous State of Host

Once the host is back online, ssh to back to *node3.example.com* and verify that the “importantfile” and “importantuser” exist in the backup snapshot:

.[root@workstation]#
----
ssh root@node3.example.com
----

.[root@node3]#
----
df /
----

.Command Output
[source,indent=4]
----
Filesystem                     1K-blocks    Used Available Use% Mounted on
/dev/mapper/rhel-root_snapshot   8374272 1321268   7053004  16% /
----

Let's find our data.

.[root@node3]#
----
cat /root/importantfile

grep importantuser /etc/passwd

du -sh /usr/share/doc

du -sh /usr/share/GeoIP

man bash
----

Wahoo! Man pages are back!  It should be clear that the data removed earlier is still present within the snapshot volume.  Now it's time to recover the data.  

NOTE: You can technically initiate the logical volume merge now, set the grub default back to the normal boot entry, and reboot as merging requires a unmount before anything happens and once it's initiated it can work in the background.  But, we are going to utilize the rescue image for extra fun!

== Return Host to Previous State

WARNING: Bring up the virtual machine console for node3 before proceeding.  

We will now reboot node3 virtual machine again into rescue mode and return the host to it's previous state.

.[root@node3]#
----
cheat-boom-grublist.sh
----

.Command Output
[source,indent=4]
----
0  title="root LV snapshot"
1  title="Red Hat Enterprise Linux (4.18.0-67.el8.x86_64) 8.0 (Ootpa)"
2  title="Red Hat Enterprise Linux (0-rescue-e988045b45b04b11b84741d6a568861b) 8.0 (Ootpa)"
----

We want to reboot to recuse mode, so in this case we use '2'.

.[root@node3]#
----
grub2-set-default 2
----

Let's go...

.[root@node3]#
----
reboot
----

Once the host is back online, ssh to back to *node3.example.com*.

.[root@workstation]#
----
ssh root@node3.example.com
----

.[root@node3]#
----
df /

cat  /proc/cmdline
----

The output shows that we are no longer mounted to the snapshot volume, and we ARE using the rescue kernel image.

.Command Output
[source,indent=4]
----
Filesystem            1K-blocks    Used Available Use% Mounted on
/dev/mapper/rhel-root   8374272 1230660   7143612  15% /

.[root@node3 ~]# cat /proc/cmdline
BOOT_IMAGE=(hd0,msdos1)/vmlinuz-0-rescue-e988045b45b04b11b84741d6a568861b root=/dev/mapper/rhel-root ro crashkernel=auto resume=/dev/mapper/rhel-swap rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet
----

Time return the host back to it's previous state.

.[root@node3]#
----
lvconvert --merge /dev/rhel/root_snapshot
----

As pointed out earlier, the "merge" will being the next time the root volume is mounted.  This output merely confirms this.

.Command Output
[source,indent=4]
----
Delaying merge since origin is open.
Merging of snapshot rhel/root_snapshot will occur on next activation of rhel/root.
----

Last few steps and we will be done.

Set grub to boot default OS again.

.[root@node3]#
----
cheat-boom-grublist.sh
----

.Command Output
[source,indent=4]
----
0  title="root LV snapshot"
1  title="Red Hat Enterprise Linux (4.18.0-67.el8.x86_64) 8.0 (Ootpa)"
2  title="Red Hat Enterprise Linux (0-rescue-e988045b45b04b11b84741d6a568861b) 8.0 (Ootpa)"
----

This time we want entry '1'.

.[root@node3]# 
----
grub2-set-default 1
----

.[root@node3]#
----
reboot
----

== Confirm System Restored

We have now returned the host to it's previous state (ie: the moment we created the snapshot with boom). Let's make sure everything is where we expect.

.[root@workstation ~]#
----
ssh root@node3.example.com
----

.[root@node3]#
----
cat /root/importantfile

grep importantuser /etc/passwd
----

.Command Output
[source,indent=4]
----
This is an important file!!
----

Last piece of information.  Since the "snapshot" was merged back into it's "origin", the snapshot itself is now gone.  You can confirm this by running "lvs" and noting that the root_snapshot is missing.

.[root@node3]#
----
lvs
----

Thus our boom-boot entry points to a non-existent volume.  Here are the final commands to clean everything up.

.[root@node3]#
----
boom entry list
----

.Command Output
[source,indent=4]
----
BootID  Version                  Name                     RootDevice
85e739d 4.18.0-67.el8.x86_64     Red Hat Enterprise Linux /dev/rhel/root_snapshot
----

Make note of the BootID and use it in the next command.

.[root@node3]#
----
boom entry delete 85e739d
----

Double check the grub configuration.

.[root@node3]#
----
cheat-boom-grublist.sh
----

Note that the snapshot entry has been removed.

.Command Output
[source,indent=4]
----
0  title="Red Hat Enterprise Linux (4.18.0-67.el8.x86_64) 8.0 (Ootpa)"
1  title="Red Hat Enterprise Linux (0-rescue-e988045b45b04b11b84741d6a568861b) 8.0 (Ootpa)"
----

Make sure grub is configured to boot the entry.

.[root@node3]#
----
grub2-editenv list
----

.Command Output
[source,indent=4]
----
saved_entry=1
kernelopts=root=/dev/mapper/rhel-root ro crashkernel=auto resume=/dev/mapper/rhel-swap rd.lvm.lv=rhel/root rd.lvm.lv=rhel/swap rhgb quiet
boot_success=0
----

Whoops!  That's not right

.[root@node3]#
----
grub2-set-default 0
----

Wahoo! You are done.  If you have any questions, please ask.

== Additional Resources

    * link:https://systemd.io/BOOT_LOADER_SPECIFICATION.html[Boot Loader Specification]

[discrete]
== End of Unit

link:../RHEL8-Workshop.adoc#toc[Return to TOC]

////
Always end files with a blank line to avoid include problems.
////
