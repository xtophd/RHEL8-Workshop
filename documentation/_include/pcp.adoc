:sectnums:
:sectnumlevels: 3
:markup-in-source: verbatim,attributes,quotes
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
endif::[]
:imagesdir: ./_images

:toc:
:toclevels: 1

= Performance Co-Pilot, Grafana, and bpftrace

== Overview

Performance Co-Pilot is a suite of tools, services, and libraries for monitoring, visualizing, storing, and analyzing system-level performance measurements. Starting with RHEL 8.2, we are delivering version 5.0.2, which has many enhancements, including providing link:https://openmetrics.io[OpenMetrics] compliant data over pmproxy that can be used by other collectors such as prometheus.

RHEL 8 also provides Grafana, a graphing, visualization tool, which we integrate with Performance Co-Pilot to graph metrics in real time.

Combined with bpftrace, we are able to get at very interesting data from the kernel.

== Getting Started

For these exercises, you will primarily be using the hosts `node2` and `node3`  as user `root`.  However, these exercises do REQUIRE a second terminal session to run commands on other hosts.  Please pay careful attention to what commands to run on different hosts.

TIP: Now is a great time to use the multi session capability of *tmux*.  Use `Ctrl-b "` to create another session with a split screen.  Cycle the active session back and forth with `CTRL-b n` (next) and `CTRL-b p` (previous).

From host `workstation`, ssh to `node2`.

[bash,options="nowrap",subs="{markup-in-source}"]
----
$ *ssh node2*
----

Use `sudo` to elevate your priviledges.

[bash,options="nowrap",subs="{markup-in-source}"]
----
$ *sudo -i*
----

Verify that you are on the right host for these exercises.

[bash,options="nowrap",subs="{markup-in-source}"]
----
# *cheat-pcp-checkhost.sh*
----

You are now ready to proceed with these exercises.

== Installation

Start by installing pcp,grafana,redis,js-d3-flame-graph, and others:

[bash,options="nowrap",subs="{markup-in-source}"]
----
# *yum install pcp grafana grafana-pcp cockpit-pcp bpftrace bcc-tools pcp-pmda-bpftrace cyrus-sasl-md5 cyrus-sasl-lib redis js-d3-flame-graph -y*
----

== pminfo and pmrep

Since we now have pcp installed and running on our machines, let's run the following on node1:

[bash,options="nowrap",subs="{markup-in-source}"]
----
pminfo | grep kernel | head -n 15
----

This will return similar output to:

[bash,options="nowrap",subs="{markup-in-source}"]
----
kernel.all.load
kernel.all.intr
kernel.all.pswitch
kernel.all.sysfork
kernel.all.running
kernel.all.blocked
kernel.all.boottime
kernel.all.hz
kernel.all.uptime
kernel.all.idletime
kernel.all.nusers
kernel.all.nroots
kernel.all.nsessions
kernel.all.lastpid
kernel.all.runnable
----

This represents 15 pcp metrics that have the word "kernel" in them. Let's use the `pmrep` command to see what the load on our system currently is:

[bash,options="nowrap",subs="{markup-in-source}"]
----
pmrep kernel.all.load -s 15
----

This will return output similar to:

[bash,options="nowrap",subs="{markup-in-source}"]
----
  k.a.load  k.a.load  k.a.load
  1 minute  5 minute  15 minut
                              
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
     0.010     0.010     0.000
----

This gives us 15 samples at a 1-second interval for the 1-minute, 5-minute, and 15-minute load average for this host. In order to make this more interesting, we can start stress-ng in the second terminal:

[bash,options="nowrap",subs="{markup-in-source}"]
----
stress-ng --aio 40 --cpu 20 &
----

and back in the first terminal:

[bash,options="nowrap",subs="{markup-in-source}"]
----
pmrep kernel.all.load -s 15
----

and our output will show more load this time:

[bash,options="nowrap",subs="{markup-in-source}"]
----
  k.a.load  k.a.load  k.a.load
  1 minute  5 minute  15 minut
                              
     1.600     0.330     0.110
     1.600     0.330     0.110
     1.600     0.330     0.110
     1.600     0.330     0.110
     3.080     0.660     0.210
     3.080     0.660     0.210
     3.080     0.660     0.210
     3.080     0.660     0.210
     3.080     0.660     0.210
     4.430     0.980     0.320
     4.430     0.980     0.320
     4.430     0.980     0.320
     4.430     0.980     0.320
     4.430     0.980     0.320
     5.680     1.300     0.430
----

Now in the second terminal, run:

[bash,options="nowrap",subs="{markup-in-source}"]
----
killall -9 stress-ng
----

This stops our stress-ng process and will allow the load on the system to return to normal.

Also, with pmrep, we can create ini files that allow us to build pcp reports. A pcp report to measure memory utilization would look like:

[bash,options="nowrap",subs="{markup-in-source}"]
----
[mem-util]
timestamp=yes
interval=1s
mem.util.bufmem=
mem.util.cached=
mem.util.free=
mem.util.used=
----

This is from the file `rhel-use.conf` and if we run this report, we will have timestamped data sampled at a 1 second interval that will include the pcp metrics: `mem.util.bufmem`, `mem.util.cached`, `mem.util.free`, `mem.util.used`. Let's use `pmrep` and rhel-use.conf to sample memory. But first, in our second terminal, let's generate some memory load:

[bash,options="nowrap",subs="{markup-in-source}"]
----
stress-ng --vm 20 &
----

and in the first terminal:

[bash,options="nowrap",subs="{markup-in-source}"]
----
pmrep -c rhel-use.conf :mem-util -s 15
----

This will print output similar to:

[bash,options="nowrap",subs="{markup-in-source}"]
----
          m.u.bufmem  m.u.cached  m.u.free  m.u.used
               Kbyte       Kbyte     Kbyte     Kbyte
20:58:49           0     1589892     89752   1770152
20:58:50           0     1502764    159928   1699976
20:58:51           0     1628788     52140   1807764
20:58:52           0     1587184     57004   1802900
20:58:53           0     1601804     76884   1783020
20:58:54           0     1610292     72720   1787184
20:58:55           0     1555400    120180   1739724
20:58:56           0     1002124    649464   1210440
20:58:57           0     1629744     52336   1807568
20:58:58           0     1621704     52664   1807240
20:58:59           0     1614144     75640   1784264
20:59:00           0     1612628     53104   1806800
20:59:01           0     1584568    105372   1754532
20:59:02           0     1602292     87688   1772216
20:59:03           0     1606756     67600   1792304
----

Now in our second terminal, let's run:

[bash,options="nowrap",subs="{markup-in-source}"]
----
killall -9 stress-ng
----

From this, we can see that pmrep is a powerful tool for reporting on pcp metrics at the command line.

== pmseries enablement

pmseries allows pcp to store data in a redis database. This allows for searching historical data at a later time, which can be quite useful during the post mortem of a performance event. 

To set up pmseries, on node2, please edit /etc/pcp/pmseries/pmseries.conf and make sure the following under `[pmproxy]` is set:

----
# support Redis protocol proxying
redis.enabled = true
----

Then make sure that under `[pmseries]`, you have the following set:

----
# allow REST API queries of fast, scalable time series
enabled = true
----

Once these are both set, save the file and run:

----
systemctl restart pmcd pmlogger pmproxy
----

It will take pmseries a few minutes to have data collected, but once this has began, you can run:

----
pmseries kernel.all.load
----

and get output similar to:

----
fc13f67815676cd1ed1687fe55030c9e8c33b059
----

which verifies that pmseries is storing data in redis. We will view this data in grafana.

== bpftrace pmda Initial Setup

Let's install the bpftrace pmda to examine how this works. The rpm is already installed on the system, but we still have to install the pmda into pcp. On node2, run:

----
cd /var/lib/pcp/pmdas/bpftrace
./Install
pmrep bpftrace.scripts.runqlat.data.usecs -s 5
----

This should return 5 samples of run queue latency measured in microseconds:

----
  b.s.r.data_bytes
            byte/s
               N/A
           570.033
           573.032
           572.621
           573.303
           573.271
           572.550
           573.998
           574.196
           575.521
           572.610
           574.096
           575.030
           577.681
           577.983
----

Now how did that happen? If we examine the `/var/lib/pcp/pmdas/bpftrace/autostart` directory, we will see two included bpftrace scripts:

----
ls -lah /var/lib/pcp/pmdas/bpftrace/autostart/
----

----
total 8.0K
drwxr-xr-x. 2 root root  45 Aug  6 20:10 .
drwxr-xr-x. 4 root root 161 Aug 13 21:24 ..
-rw-r--r--. 1 root root 601 Jun 23 05:21 biolatency.bt
-rw-r--r--. 1 root root 794 Jun 23 05:21 runqlat.bt
----

If we look at runqlat.bt, we will see a line in the code that reads:

----
                @usecs = hist((nsecs - $ns) / 1000);
----

This pmda has converted this `@usecs` bpfmap to a pcp metric. To see all pcp metrics from this script, run:

----
pminfo | grep bpftrace | grep runqlat
----

and you will see:

----
bpftrace.scripts.runqlat.data.usecs
bpftrace.scripts.runqlat.data_bytes
bpftrace.scripts.runqlat.code
bpftrace.scripts.runqlat.probes
bpftrace.scripts.runqlat.error
bpftrace.scripts.runqlat.exit_code
bpftrace.scripts.runqlat.pid
bpftrace.scripts.runqlat.status
----

As such, any bpftrace script placed in the "autostart" directory will be parsed, run, and made available through pcp in this manner. If you add a bpftrace script, you do need to run `Remove` followed by `Install` in the `/var/lib/pcp/pmdas/bpftrace/` directory for this script to be picked up. This makes for a powerful integration between pcp and bpftrace.

== Remote Logging Setup

Let's use our pcp node2 as a remote logging server and our pcp node3 as a client. To do this, let's go to node3 and set up pmlogger as a client:

Edit /etc/sysconfig/pmcd and make sure PMCD_LOCAL=0

----
# Behaviour regarding listening on external-facing interfaces;
# unset PMCD_LOCAL to allow connections from remote hosts.
# A value of 0 permits remote connections, 1 permits local only.

PMCD_LOCAL=0
----

Save this file and then we will need to open some services on the firewall:

----
firewall-cmd --add-service=pmproxy --add-service=pmcd --permanent
firewall-cmd --reload
----

Now we need to allow pcp to bind to unreserved ports:

----
setsebool -P pcp_bind_all_unreserved_ports on
----

Let's restart pcp:

----
systemctl restart pmcd pmlogger
----

Now back on node2, let's set up remote logging for node3:

Edit /etc/pcp/pmlogger/control.d/remote and add:
----
node3 n n PCP_LOG_DIR/pmlogger/node3 -r T24h10m -c config.remote
----

Now, let's restart pcp:

----
systemctl restart pmcd pmlogger
----

Let's verify that we are now getting logs from node3:

----
cd /var/log/pcp/pmlogger/node3
for i in $(ls *.0); do pmdumplog -L $i; done
----

This should generate output similar to:

----
Log Label (Log Format Version 2)
Performance metrics from host node3
    commencing Thu Aug 13 21:39:06.614021 2020
    ending     Thu Aug 13 21:39:07.322042 2020
Archive timezone: CEST-2
PID for pmlogger: 5595
Log Label (Log Format Version 2)
Performance metrics from host node3
    commencing Thu Aug 13 21:39:15.271834 2020
    ending     Thu Aug 13 21:39:15.307463 2020
Archive timezone: CEST-2
PID for pmlogger: 6842
----

If you something like the above, then you have successfully set up remote logging. node2 is now accepting remote logs from node3 and further, metrics for node3 and node2 are being stored in the pmseries redis database!

== Grafana Integration

Let's go ahead and setup Grafana on node2:

----
systemctl enable grafana-server
systemctl start grafana-server
firewall-cmd --add-service=grafana --add-service=pmproxy --permanent
firewall-cmd --reload
firewall-cmd --list-services
----

This last command should show both `pmproxy` and `grafana` in the open services:

----
cockpit dhcpv6-client grafana http https pmproxy ssh
----

Now from your browser, go to link:http://node2:3000[http://node2:3000] and login with the username `admin` and the password `admin`. You will be asked to specify a new password for the `admin` account. Please remember what you set the password to.

image::grafana-10-login.png[Login]

Once you've logged in, click on the configuration cog and select "Plugins":

image::grafana-15-plugins.png[Configure Plugins]

Now search for "Performance" and click on the "Performance Co-Pilot" plugin:

image::grafana-20-pcp-plugin.png[Performance Co-Pilot Grafana Plugin]

Now click "Enable":

image::grafana-25-pcp-enable.png[Enable Performance Co-Pilot Plugin]

Now we can click on the configuration cog and select "Data Sources":

image::grafana-30-datasources.png[Data Sources]

Once this comes up, you'll be presented with a button for "Add Data Source":

image::grafana-35-adddatasource.png[Add Data Source]

Click on "Add Data Source" and then search for "pcp":

image::grafana-40-datasources-pcp.png[Searching for pcp data sources]

Let's start by clicking "Select" next to PCP Redis. This will bring us to the following configuration page, where will specify `http://localhost:44322` for the URL:

image::grafana-45-pcpredis-config.png[PCP Redis Config]

Now we'll hit "Save & Test":

image::grafana-50-saveandtest.png[Save and Test]

Now click on the configuration cog and select "Data Sources" again. At this time, we'll see that the "Add Data Source" button has moved:

image::grafana-55-addagain.png[Add Button has Moved]

Click on "Add Data Source", search for pcp and repeat the above steps for:

* PCP Vector
* PCP bpftrace

Once you have finished this, click on the Dashboards icon and select "Manage":

image::grafana-60-managebutton.png[Manage Button]

From here you will see a list of dashboards that you can click on:

image::grafana-65-managedashboards.png[Manage Dashboards]

This is what our "PCP Vector Host Overview" Dashboard looks like:

image::grafana-70-vectordashboard.png[Vector Dashboard]

This is what our "PCP Redis Host Overview" Dashboard looks like:

image::grafana-75-redisdashboard.png[Redis Dashboard]

Congratulations, you've set up grafana to work with performance co-pilot!

== bpftrace and grafana Integration

In the last unit, we set up grafana and started to configure bpftrace, but didn't do anything with it. This is because this integration gives root level access to a grafana user! We will now explore this integration. Red Hat strongly advises not using this integration in production. The perfect use case for this is to use it in development and when you get your bpftrace scripts solid with you graphs, add those bpftrace scripts to the `autostart` directory of the bpftrace pmda and then expose the metrics via Vector, which will do in the next unit.

Because we do not want to allow anyone to use the bpftrace integration without authentication, we are going to enable a sysadmin user to authenticate from grafana to pcp for the purpose of using the bpftrace pmda. 

On node2, let's look at `/etc/sasl/pmcd.conf`. We are specifically looking to make sure that `digest-md5` is in the mech_list and that the sasldb_path is set to `/etc/pcp/passwd.db`. The file should look like:

----
# Enabled authentication mechanisms (space-separated list).
# You can list many mechanisms at once, then the user can choose
# by adding e.g. '?authmech=gssapi' to their host specification.
# For other options, refer to SASL pluginviewer command output.
#mech_list: plain login digest-md5 gssapi
mech_list: plain login digest-md5

# If deferring to the SASL auth daemon (runs as root, can do PAM
# login using regular user accounts, unprivileged daemons cannot).
#pwcheck_method: saslauthd

# If using plain/digest-md5 for user database, this sets the file
# containing the passwords.  Use 'saslpasswd2 -a pmcd [username]'
# to add entries and 'sasldblistusers2 -f $sasldb_path' to browse.
# Note: must be readable as the PCP daemons user (chown root:pcp).
sasldb_path: /etc/pcp/passwd.db

# Before using Kerberos via GSSAPI, you need a service principal on
# the KDC server for pmcd, and that to be exported to the keytab.
#keytab: /etc/pcp/krb5.tab
----

As we can see, everything is set correctly and there is nothing for us to do here.

Now let's set up our sysadmin user on node2:

----
useradd -r sysadmin
passwd sysadmin
saslpasswd2 -a pmcd sysadmin
----

You do not have to set the regular password and the sasl password to be the same. That said, you will need the sasl password in a few steps, so remember what you set it to!

Now we need to set the correct ownership and permission on the pcp sasl2 database on node2:

----
chown root:pcp /etc/pcp/passwd.db
chmod 640 /etc/pcp/passwd.db
----

Now we need to restart pmcd on node2:

----
systemctl restart pmcd
----

Now on node2, let's verify that sasl authentication is working:

----
pminfo -f h "pcp://127.0.0.1?username=sysadmin" disk.dev.read
----

You will be prompted for your password and if you enter it correctly and sasl is set up correctly, you will see output similar to:

----
disk.dev.read
    inst [0 or "vda"] value 10664
----

Now we need to change the bpftrace configuration to allow the sysadmin user access. On node2, edit `/var/lib/pcp/pmdas/bpftrace/bpftrace.conf` and make sure the following are set under `[dynamic_scripts]`:

----
enabled = true
allowed_users = root,sysadmin
----

Once these changes have been made, run these commands on node2 to re-install the bpftrace pmda:

----
cd /var/lib/pcp/pmdas/bpftrace
./Remove
./Install
pminfo | grep bpf
----

On that last pminfo command, you should see bpf metrics showing like:

----
bpftrace.scripts.runqlat.data.usecs
bpftrace.scripts.runqlat.data_bytes
bpftrace.scripts.runqlat.code
bpftrace.scripts.runqlat.probes
bpftrace.scripts.runqlat.error
bpftrace.scripts.runqlat.exit_code
bpftrace.scripts.runqlat.pid
bpftrace.scripts.runqlat.status
...
----

Now back in our Grafana dashboard, click on the Configuration cog and "Data Sources":

image::grafana-30-datasources.png[Data Sources]

and then click on the "PCP bpftrace" data source. On this page, click the toggle for "Basic auth" under the "Auth" section so that it is on. You will know it is on when you get a "Basic Auth Details" section where you will enter the sasl credentials for the `sysadmin` user:

image::grafana-80-bpftrace-auth.png[bpftrace Auth]

Click "Save & Test" and you should get the "Data source is working" message. After this, you can click on the Dashboards icon, select "Manage" and pick the "PCP bpftrace System Analysis" dashboard, which looks like this:

image::grafana-85-bpftrace-system-analysis.png[bpftrace System Analysis]

Now let's click the Dashboards icon, select "Manage" and pick the "PCP bpftrace Flame Graphs" dashboard, which looks like:

image::grafana-90-bpftrace-flame-graphs.png[bpftrace Flame Graphs]

This let's us see live flame graphs of on cpu activity. To read a flame graph, you read from the bottom to the top. At the bottom, you have the process name and pid number. Above that, you will see the stack graphed out by function. The wider a process/function is on the flame graph, the more cpu it is taking. The redder a process/function is on the flame graph, the more cpu cycles it is spinning. Flame graphs provide a great visualization to understand how a CPU is spending its time.

== Visualizing bpftrace data without providing root access to a user

So while it's nice that we can now see bpftrace data visualized in grafana, we've also given this user great power over the system. Let's click on the Dashboards icon, select "Manage" and pick the "PCP bpftrace System Analysis" dashboard again. Once there, click on "CPU Usage" and then click "Edit":

image::grafana-95-bpftrace-cpuedit.png[Edit CPU Chart]

After this, you'll be taken to a screen where you can edit and run live bpftrace code on the system. This is the part you don't want to have exposed in production!

image::grafana-100-bpftrace-cpuwalk-bt.png[Editing cpuwalk.bt in Grafana]

This is the cpuwalk.bt file and you can make changes to it here and see those changes live in the graph. We are not going to make changes, but we would like to expose this data to our end users without giving them access to run their own bpftrace scripts. Let's save the contents of this file out to /root/cpuwalk.bt on our node2 system.

Now on node2, run:

----
cd /var/lib/pcp/pmdas/bpftrace/autostart
cp /root/cpuwalk.bt .
restorecon ./cpuwalk.bt
ls -lahZ .
----

Once you have done the above you should see the file in this directory with these permissions:

----
-rw-r--r--. 1 root root unconfined_u:object_r:pcp_var_lib_t:s0 497 Aug 17 17:17 cpuwalk.bt
----

Now, we need to remove and re-install the bpftrace pmda:

----
cd /var/lib/pcp/pmdas/bpftrace
./Remove
./Install
pminfo | grep bpftrace | grep cpuwalk
----

If successful, we should see this output:

----
bpftrace.scripts.cpuwalk.data.output
bpftrace.scripts.cpuwalk.data.cpu
bpftrace.scripts.cpuwalk.data_bytes
bpftrace.scripts.cpuwalk.code
bpftrace.scripts.cpuwalk.probes
bpftrace.scripts.cpuwalk.error
bpftrace.scripts.cpuwalk.exit_code
bpftrace.scripts.cpuwalk.pid
bpftrace.scripts.cpuwalk.status
----

Now, we can access the bpf map named `@cpu` through the pcp metric `bpftrace.scripts.cpuwalk.data.cpu`. So let's do this in grafana. Click the Dasboard icon, select Manage, and then click the "PCP Vector Host Overview" dashboard. On this dashboard, click "CPU%" and click "Edit":

image::grafana-105-vector-cpuedit.png[Edit CPU% Script]

Now you will see the following screen:

image::grafana-110-vector-cpuquery.png[CPU% Query]

On this screen, delete the "B" query and change the "A" query to read:

----
bpftrace.scripts.cpuwalk.data.cpu
----

When you have done this, your screen will look like:

image::grafana-115-vector-cpuedit2.png[Now Using bpf!]

and you can see the data from the bpftrace script cpuwalk.bt being renedered through PCP Vector without giving the grafana user direct access to eBPF!


== Additional Resources

NOTE: You are not required to reference any additional resources for these exercises.  This is informational only.

    * link:http://www.brendangregg.com/ebpf.html[Linux Extended BPF (eBPF Tracing Tools) - Brendan Gregg]
    * link:https://github.com/xdp-project/xdp-tutorial[Upstream XDP Tutorial (eXpress Data Path networking is tech preview in RHEL 8.2.)]
    * link:https://developers.redhat.com/blog/tag/ebpf/[eBPF blogs on Red Hat Developer (covering the networking aspect)]

[discrete]
== End of Unit

ifdef::env-github[]
link:../RHEL8-Workshop.adoc#toc[Return to TOC]
endif::[]

////
Alway end files with a blank line to avoid include problems.
////
